<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>模拟面试问答之经典网络模型一：RCNN</title>
    <url>/2020/06/13/RCNN/</url>
    <content><![CDATA[<p>本文总结了面试过程中可能问到的关于RCNN模型的一些问题。</p>
<p><a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">论文地址：Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</a><br><img src="/2020/06/13/RCNN/1.png" alt="RCNN网络结构"></p>
<center><b>图1 RCNN网络结构图</b></center>

<a id="more"></a>

<h1 id="简述一下RCNN测试时的检测流程？"><a href="#简述一下RCNN测试时的检测流程？" class="headerlink" title="简述一下RCNN测试时的检测流程？"></a>简述一下RCNN测试时的检测流程？</h1><ol>
<li>使用Selective Search在输入图像上提取约2000个候选区域。</li>
<li>缩放每个候选区域到固定尺寸大小，227×227。</li>
<li>将每个候选区域分别输入CNN网络提取特征。</li>
<li>使用二分类SVM分别对每个候选区域进行分类。</li>
<li>对每个类别进行NMS（非极大抑制）。</li>
<li>使用线性回归器分别修正每个候选区域的位置。</li>
</ol>
<h1 id="RCNN是如何对候选区域进行缩放的？"><a href="#RCNN是如何对候选区域进行缩放的？" class="headerlink" title="RCNN是如何对候选区域进行缩放的？"></a>RCNN是如何对候选区域进行缩放的？</h1><p>各向异性缩放且padding=16。</p>
<h1 id="分别介绍一下各向异性和各项同性的缩放？"><a href="#分别介绍一下各向异性和各项同性的缩放？" class="headerlink" title="分别介绍一下各向异性和各项同性的缩放？"></a>分别介绍一下各向异性和各项同性的缩放？</h1><ol>
<li>各向异性直接把图像的宽高拉伸到目标尺寸。</li>
<li>各项同性先按原图像比例进行缩放，然后在再填充到目标尺寸。</li>
</ol>
<h1 id="RCNN的CNN网络是如何训练的？"><a href="#RCNN的CNN网络是如何训练的？" class="headerlink" title="RCNN的CNN网络是如何训练的？"></a>RCNN的CNN网络是如何训练的？</h1><p><img src="/2020/06/13/RCNN/2.jpg" alt="AlexNet网络结构"></p>
<center><b>图2 AlexNet网络结构图</b></center>

<ol>
<li>使用AlexNet(网络结构如<strong>图2</strong>所示)或其他主干网络（如VGG16），先在ILSVRC2012的图像分类数据集上进行预训练，最后一层输出维度为1000。</li>
<li>将AlexNet最后的1000维Softmax层替换为随机初始化的20+1维Softmax层，其余层使用预训练得到的参数进行初始化，在PASCAL VOC数据集上对预训练后的AlexNet进行fine-tuning（微调）。</li>
<li>Fine-tuning时，定义与ground-truth的IoU大于0.5的候选区域为正样本，否则为负样本（即背景样本），在每次迭代时，采样32个正样本和96个负样本组成一个128的mini-batch。</li>
</ol>
<h1 id="RCNN的SVM分类器是如何训练的？"><a href="#RCNN的SVM分类器是如何训练的？" class="headerlink" title="RCNN的SVM分类器是如何训练的？"></a>RCNN的SVM分类器是如何训练的？</h1><ol>
<li>共训练20个二分类SVM。</li>
<li>使用fine-tuning训练后且去掉最后softmax层的AlexNet提取候选区域的特征作为SVM的输入。</li>
<li>定义IoU小于0.3的候选区域为负样本，并进行难负样本挖掘，完整包含物体（IoU=1）的候选区域为正样本，其余样本舍弃。</li>
</ol>
<h1 id="RCNN的线性回归器是如何训练的？"><a href="#RCNN的线性回归器是如何训练的？" class="headerlink" title="RCNN的线性回归器是如何训练的？"></a>RCNN的线性回归器是如何训练的？</h1><ol>
<li>共训练20×4个回归器，正则项系数$\lambda$=10000。</li>
<li>输入为AlexNet的第一个全连接层的4096维输出（这里有点问题，论文里面说的是Pool 5，而我理解的Pool 5的输出时6×6×256的），输出为中心点坐标的平移量和宽高的缩放值。</li>
<li>定义IoU大于0.6的样本作为正样本用于训练。</li>
</ol>
<h1 id="Fine-tuning后的AlexNet的输出为21维，已经对各个候选区域进行了分类，为什么还要使用SVM进行分类呢？（或为什么不在AlexNet最后使用Softmax进行分类？）"><a href="#Fine-tuning后的AlexNet的输出为21维，已经对各个候选区域进行了分类，为什么还要使用SVM进行分类呢？（或为什么不在AlexNet最后使用Softmax进行分类？）" class="headerlink" title="Fine-tuning后的AlexNet的输出为21维，已经对各个候选区域进行了分类，为什么还要使用SVM进行分类呢？（或为什么不在AlexNet最后使用Softmax进行分类？）"></a>Fine-tuning后的AlexNet的输出为21维，已经对各个候选区域进行了分类，为什么还要使用SVM进行分类呢？（或为什么不在AlexNet最后使用Softmax进行分类？）</h1><ol>
<li>对AlexNet进行fine-tuning时采用的IoU阈值较小，被分为正样本的候选区域可能只包含部分物体，分类精度较低。</li>
<li>SVM训练时定义完全包含物体的候选区域才是正样本，分类精度较高。因此，使用SVM能够提高分类精度。</li>
</ol>
<h1 id="为什么fine-tuning训练时和SVM训练时的IoU阈值不一样？"><a href="#为什么fine-tuning训练时和SVM训练时的IoU阈值不一样？" class="headerlink" title="为什么fine-tuning训练时和SVM训练时的IoU阈值不一样？"></a>为什么fine-tuning训练时和SVM训练时的IoU阈值不一样？</h1><ol>
<li>Fine-tuning训练时，若IoU阈值设置过高，则可用于的训练样本较少（正样本减少，负样本为正样本的三倍），并且此时AlexNet已进行预训练，易由于样本过少而产生过拟合。因此，设置0.5的IoU阈值只是为了增加可用于训练的样本数量，避免过拟合。  </li>
<li>SVM训练时，为了达到进一步提高分类精度的目的，需要设置更为严格的正负样本分类规则。</li>
</ol>
<h1 id="回归器是如何进行边界框回归的？"><a href="#回归器是如何进行边界框回归的？" class="headerlink" title="回归器是如何进行边界框回归的？"></a>回归器是如何进行边界框回归的？</h1><h2 id="简答"><a href="#简答" class="headerlink" title="简答"></a>简答</h2><p>通过预测候选区域中心点坐标的平移量$\Delta x$，$\Delta y$和宽高的缩放值$S_w$，$S_h$来回归边界框。因此对于每个类别，需要4个回归器分别回归$\Delta x$，$\Delta y$，$S_w$，$S_h$，整个模型就包括20×4个回归器。</p>
<h2 id="详细"><a href="#详细" class="headerlink" title="详细"></a>详细</h2><p>定义$P=(P_x,P_y,P_w,P_h)$表示候选区域的中心坐标和宽高，$G=(G_x,G_y,G_w,G_h)$表示ground-truth的中心坐标和宽高。预测$d_*(P)$（$*$表示$x$，$y$，$w$，$h$，下同）得到平移量$\Delta x=P_wd_x(P)$，$\Delta y=P_hd_y(P)$和缩放值$S_w=exp(d_w(P))$，$S_h=exp(d_h(P))$，目标是使变换后的P与G更接近。<br>定义$\widehat{G}=(\widehat{G}_x,\widehat{G}_y,\widehat{G}_w,\widehat{G}_h)$表示变换后的$P$，其中：<br>$$\widehat{G}_x=P_wd_x(P)+P_x$$<br>$$\widehat{G}_y=P_hd_y(P)+P_y$$<br>$$\widehat{G}_w=P_wexp(d_w(P))$$<br>$$\widehat{G}_h=P_hexp(d_h(P))$$<br>回归器的输入是AlexNet提取的候选区域特征，表示为$\phi(P)$，令$d_*(P)=w^T_*\Phi(P)$，$w_*$就是需要学习的线性变换参数了，可以得到：<br>$$w^T_x\Phi(P)=d_x(P)=(\widehat{G}_x-P_x)/P_w$$<br>$$w^T_y\Phi(P)=d_y(P)=(\widehat{G}_y-P_y)/P_h$$<br>$$w^T_w\Phi(P)=d_w(P)=log(\widehat{G}_w/P_w)$$<br>$$w^T_h\Phi(P)=d_h(P)=log(\widehat{G}_h/P_h)$$<br>定义$t_*$为：<br>$$t_x=(G_x-P_x)/P_w$$<br>$$t_y=(G_y-P_y)/P_h$$<br>$$t_w=log(G_w/P_w)$$<br>$$t_h=log(G_h/P_h)$$<br>目标是使P的映射$\widehat{G}$尽量接近$G$，通过以下优化损失函数求解$w_*$：<br>$$w_*=\underset{\widehat{w}_*}{argmin}\sum(t_*^i-\widehat{w}_*^T\phi(P^i))^2+\lambda&#124;&#124;\widehat{w}_*&#124;&#124;^2$$<br>可通过最小二乘法或梯度下降求解该问题。由此也可以看出，对于每个类别，需要4个回归器分别回归$\Delta x$，$\Delta y$，$S_w$，$S_h$，整个模型就包括20×4个回归器。</p>
<h1 id="回归器为什么不直接预测平移量-Delta-x-和-Delta-y-的值，而是先预测-d-x-P-和-d-y-P-，然后分别乘以-P-w-和-P-h-，间接求出-Delta-x-和-Delta-y-？"><a href="#回归器为什么不直接预测平移量-Delta-x-和-Delta-y-的值，而是先预测-d-x-P-和-d-y-P-，然后分别乘以-P-w-和-P-h-，间接求出-Delta-x-和-Delta-y-？" class="headerlink" title="回归器为什么不直接预测平移量$\Delta x$和$\Delta y$的值，而是先预测$d_x(P)$和$d_y(P)$，然后分别乘以$P_w$和$P_h$，间接求出$\Delta x$和$\Delta y$？"></a>回归器为什么不直接预测平移量$\Delta x$和$\Delta y$的值，而是先预测$d_x(P)$和$d_y(P)$，然后分别乘以$P_w$和$P_h$，间接求出$\Delta x$和$\Delta y$？</h1><p>回归器的输入是候选区域的特征，而每个候选区域在提取特征后的大小时相同的，若是直接预测平移量$\Delta x$和$\Delta y$的值，那么对于两个内容相差无几，但是尺寸不一样的图像，会得到相同平移量，这显然是有问题的。而分别乘候选区域的宽高$P_w$和$P_h$相当于进行归一化，即尺寸大的候选区域预测的平移量大，尺寸小的候选区域的平移量小。</p>
<h1 id="回归器预测宽高的缩放值为什么要采用exp？"><a href="#回归器预测宽高的缩放值为什么要采用exp？" class="headerlink" title="回归器预测宽高的缩放值为什么要采用exp？"></a>回归器预测宽高的缩放值为什么要采用exp？</h1><p>保证预测值大于0.</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol>
<li>优点：RCNN引入CNN提取特征，取代手工设计特征，并提升了检测速度；采用了迁移学习的思想，先在大数据集上对模型进行预训练，然后在目标数据集上面进行fine-tuning。</li>
<li>缺点：RCNN使用Selection Search提取2000个候选区域，不够精确；需要对每个候选区域单独提取特征，无法共享特征，且内存占用和耗时较大；训练过程分为三个阶段，繁琐且耗时较大。</li>
</ol>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>经典网络</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>模拟面试问答之经典网络模型二：Fast RCNN</title>
    <url>/2020/07/01/FastRCNN/</url>
    <content><![CDATA[<p>本文总结了面试过程中可能问到的关于Fast RCNN模型的一些问题。</p>
<p><a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">论文地址：Fast R-CNN</a><br><img src="/2020/07/01/FastRCNN/1.jpg" alt="Fast RCNN网络结构"></p>
<center><b>图1 Fast RCNN网络结构图</b></center>

<a id="more"></a>

<h1 id="简述一下Fast-RCNN测试时的检测流程？"><a href="#简述一下Fast-RCNN测试时的检测流程？" class="headerlink" title="简述一下Fast RCNN测试时的检测流程？"></a>简述一下Fast RCNN测试时的检测流程？</h1><ol>
<li>使用Selective Search在输入图像上提取约2000个候选区域。</li>
<li>将原始图像输入卷积网络，获得最后一个池化层之前的特征图（16倍下采样）。</li>
<li>对于每个候选区域，RoI pooling层将候选区域定位到特征图中的对应区域产生RoI，然后将每个RoI转化为相同尺寸（7×7）。</li>
<li>通过全连接层将RoI转化为一维向量，分别输入并行的分类器和回归器。分类器由一个全连接层和Softmax组成，对每一个RoI输出20+1个类别概率。回归器输出通过一个全连接层对每一个RoI输出20×4维向量，即所有类别下回归RoI的位置和大小。</li>
</ol>
<h1 id="描述一下RoI-pooling的工作原理？"><a href="#描述一下RoI-pooling的工作原理？" class="headerlink" title="描述一下RoI pooling的工作原理？"></a>描述一下RoI pooling的工作原理？</h1><p>先将候选区域定位到特征图中的对应区域产生RoI，然后将RoI划分为H×W的区域，然后分别取每个区域的最大值代替该区域，从而将RoI尺寸调整为H×W。</p>
<h1 id="两次坐标量化是指什么？"><a href="#两次坐标量化是指什么？" class="headerlink" title="两次坐标量化是指什么？"></a>两次坐标量化是指什么？</h1><ol>
<li>第一次量化时将候选区域映射到特征图上时，需要将候选区域坐标除以16并取整。</li>
<li>第二次量化是指RoI pooling时，需要将RoI划分为H×W个部分，对左上角坐标采用向下取整，对右下角坐标采用向上取整。两次量化操作会引入误差，Mask RCNN中会解决这一问题。</li>
</ol>
<h1 id="Fast-RCNN是如何训练的？"><a href="#Fast-RCNN是如何训练的？" class="headerlink" title="Fast RCNN是如何训练的？"></a>Fast RCNN是如何训练的？</h1><ol>
<li>以VGG16为例，先在ImageNet上训练一个1000类的分类网络。</li>
<li>修改预训练后的网络结构，将最后一个池化层替换为RoI pooling层；将最后一个全连接层替换为并行的分类器和回归器；网络输入替换为图像+候选区域集合。</li>
<li>将修改后的网络在Pascal VOC上进行fine-tuning，定义IoU大于0.5的为正样本，IoU为0.1-0.5之间的为负样本，每次迭代时包括2张图像，每张图像随机采样16个正样本和48个负样本用于训练；采样0.5概率的水平翻转进行数据增强。</li>
<li>采用分类损失和回归损失的联合组成的多任务损失函数进行训练。</li>
</ol>
<h1 id="Fast-RCNN的损失函数是如何构成的？"><a href="#Fast-RCNN的损失函数是如何构成的？" class="headerlink" title="Fast RCNN的损失函数是如何构成的？"></a>Fast RCNN的损失函数是如何构成的？</h1><h2 id="简答"><a href="#简答" class="headerlink" title="简答"></a>简答</h2><p>Fast RCNN采用多任务损失函数，由分类损失和回归损失联合组成。分类采用交叉熵损失函数，回归采用Smooth L1损失函数。</p>
<h2 id="详细"><a href="#详细" class="headerlink" title="详细"></a>详细</h2><p>Fast RCNN的多任务损失函数如下：<br>$$L(p,u,t^u,v)=L_{cls}(p,u)+\lambda [u\geq 1]L_{loc}(t^u,v)$$<br>其中，$L_{cls}(p,u)=-logp_u$表示真实分类u的概率$p_u$的对数损失。$\lambda$用于平衡分类损失和回归损失，通常取1。$[u\geq 1]$为指示函数，$u\geq 1$时取1否则取0，即当p为负样本时忽略回归损失。$L_{loc}(t^u,v)$为四个Smooth L1损失的和，分别是两个平移量和两个缩放值（具体参考上一篇关于RCNN的博客）。Smooth L1损失函数如下所示：<br>$$Smooth_{L1}(x)=\begin{cases} 0.5x^2 &amp; if&#124;x&#124;&lt;1 \\ &#124;x&#124;-0.5 &amp; otherwise \end{cases}$$<br>Smooth L1损失函数在x较大时（训练初期）采取L1损失函数形式，避免了L2损失在训练初期导数大的问题；在x较小（训练后期）时采取L2损失函数形式，避免了L1损失函数在训练后期导数大的问题。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol>
<li>优点：Fast RCNN解决了RCNN的两大问题：①一次性提取特征然后对候选区域进行特征映射，实现特征共享，极大地节约了时间和空间消耗；②将RCNN的三个模型整合为一个模型，一次性进行分类和回归。</li>
<li>缺点：仍然存在与RCNN一样的问题，即使用Selective Search进行候选框提取。</li>
</ol>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>经典网络</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>模拟面试问答之经典网络模型三：Faster RCNN</title>
    <url>/2020/07/15/FasterRCNN/</url>
    <content><![CDATA[<p>本文总结了面试过程中可能问到的关于Faster RCNN模型的一些问题。</p>
<p><a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">论文地址：Faster R-CNN</a><br><img src="/2020/07/15/FasterRCNN/1.png" alt="Faster RCNN网络结构"></p>
<center><b>图1 Faster RCNN网络结构图</b></center>

<a id="more"></a>

<h1 id="简述一下Fast-RCNN测试时的检测流程？"><a href="#简述一下Fast-RCNN测试时的检测流程？" class="headerlink" title="简述一下Fast RCNN测试时的检测流程？"></a>简述一下Fast RCNN测试时的检测流程？</h1><ol>
<li>调整输入图像尺寸为16的倍数。</li>
<li>使用CNN（以VGG16为例）提取特征图。</li>
<li>RPN（Region Proposal Network）在特征图上生成一系列anchor box，并通过两个分支分别判断anchor box是否包含物体以及粗略位置修正。</li>
<li>将特征图和anchor boxes一起输入到RoI pooling层，与Fast RCNN一样，对于每个anchor boxes，RoI pooling层将候选区域定位到特征图中的对应区域产生RoI，然后将每个RoI转化为相同尺寸（7×7）。</li>
<li>经过两个全连接+ReLU层将RoI转化为一维向量，分别输入并行的分类器和回归器。分类器由一个全连接层和Softmax组成，对每一个RoI输出80+1个类别概率。回归器输出通过一个全连接层对每一个RoI输出80×4维向量，即所有类别下回归RoI的位置和大小。</li>
</ol>
<h1 id="Faster-RCNN相对于Fast-RCNN的主要改进在什么地方？"><a href="#Faster-RCNN相对于Fast-RCNN的主要改进在什么地方？" class="headerlink" title="Faster RCNN相对于Fast RCNN的主要改进在什么地方？"></a>Faster RCNN相对于Fast RCNN的主要改进在什么地方？</h1><ol>
<li>Faster RCNN不再使用Selective Search提取候选区域，而是采用卷积网络（RPN）产生候选框。</li>
<li>RPN网络在生成候选框后会对候选框进行筛选，只保留可能包含物体的候选框（～300个），为后续计算节约计算成本。</li>
</ol>
<h1 id="介绍一下RPN网络？"><a href="#介绍一下RPN网络？" class="headerlink" title="介绍一下RPN网络？"></a>介绍一下RPN网络？</h1><p><img src="/2020/07/15/FasterRCNN/3.png" alt="Faster RCNN网络结构"></p>
<center><b>图2 RPN网络结构图</b></center>

<ol>
<li>首先通过一个3×3的卷积进一步集中特征信息，然后在特征图上的每个特征点生成9个anchor box（三个尺度，三个比例），把生成的anchor boxes分别送入分类分支和回归分支。</li>
<li>对于分类分支，通过一个1×1卷积+Softmax输出每个anchor box存在物体的概率。</li>
<li>对于回归分支，通过一个1×1卷积输出每个anchor box的粗略位置估计，包括2个平移量（$\Delta x$，$\Delta y$）和两个缩放量($S_h$，$S_w$)，具体参考前面关RCNN的博客。</li>
<li>最后Proposal层根据分类和回归分支结果筛选候选框。首先根据回归结果修正候选框位置，然后根据分类结果筛选出前k（6000）个存在物体概率最高的候选框，并去除尺寸较小的和超出边界的候选框，再使用NMS剔除重叠候选框（0.7阈值），最后将筛选结果（约300个）送入RoI pooling层。</li>
</ol>
<h1 id="Faster-RCNN是如何训练的？"><a href="#Faster-RCNN是如何训练的？" class="headerlink" title="Faster RCNN是如何训练的？"></a>Faster RCNN是如何训练的？</h1><ol>
<li>交替训练（论文采用）：Faster RCNN可以看做时Fast RCNN + RPN，可以将Fast RCNN和RPN单独进行训练，但这两部分共享VGG16部分。<br>① 首先在ImageNet上面预训练VGG16；<br>② 使用①中的VGG16参数结合RPN进行训练；<br>③ 使用①中的VGG16参数组成Fast RCNN，并利用训练后的RPN网络生成候选框来训练Fast RCNN；<br>④ 使用③中的VGG16参数并结合RPN进行训练，VGG16参数不更新。<br>⑤ 使用③中的VGG16参数Fast RCNN，并利用④中的RPN网络生成候选框来训练Fast RCNN，VGG16参数不更新。</li>
<li>近似联合训练：将Faster RCNN整体进行训练，包括四个损失函数，RPN二分类损失，RPN回归损失，最后的多分类损失和回归损失。</li>
</ol>
<h1 id="Faster-RCNN的损失函数是如何构成的？"><a href="#Faster-RCNN的损失函数是如何构成的？" class="headerlink" title="Faster RCNN的损失函数是如何构成的？"></a>Faster RCNN的损失函数是如何构成的？</h1><ol>
<li>对于RPN训练，定义IoU最大或者IoU大于0.7的候选框为正样本，IoU小于0.3的候选框为负样本，其余候选框舍弃，每张图片随机采样128个正样本和128个负样本用于训练，若正样本过少，则增加负样本，保持样本总数为256。RPN的多任务损失函数为：<br>$$L({p_i},{t_i})=\frac {1}{N_{cls}}\sum_i L_{cls}(p_i,p_i^*)+\lambda \frac {1}{N_{reg}}\sum_i p_i^*L_{reg}(t_i,t_i^*)$$<br>其中，对于正样本$p_i^*=1$，否则为0。$L({p_i},{t_i})=-logp_i$表示第i个候选框含有物体的概率$p_i$的对数损失。$L_{loc}(t^u,v)$为四个Smooth L1损失的和，分别是两个平移量和两个缩放值（关于Smooth L1损失可参考上一篇关于Fast RCNN的博客）。</li>
<li>对于网络末端的分类器和回归器的训练，采用与Fast RCNN相同的损失函数。</li>
<li>实际上RPN的损失函数与Fast RCNN唯一的区别就在于一个是二分类，一个是多分类。此外，两者对于正负样本的划分有所区别。</li>
</ol>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol>
<li>优点：Faster RCNN利用RPN生成候选框，舍弃了耗时的Selective Search方法，无论是检测速度和精度都得到了提升。</li>
<li>缺点：Faster RCNN的两阶段方法无法达到实时检测的效果。</li>
</ol>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>经典网络</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux-File</title>
    <url>/2020/07/15/Linux-File/</url>
    <content><![CDATA[<p>本文总结了Linux系统中对文件属性和权限进行操作的常用命令。</p>
<a id="more"></a>

<h1 id="ls-l用于查看文件属性"><a href="#ls-l用于查看文件属性" class="headerlink" title="ls -l用于查看文件属性"></a><code>ls -l</code>用于查看文件属性</h1><pre><code>➜  ~ ls -l 
total 21776
drwxr-xr-x 26 dong dong     4096 11月 29  2019  anaconda3
drwxr-xr-x  8 dong dong     4096 7月  13 23:46  Clover</code></pre><p>每一个文件开头10个字符含义如下图：<br><img src="/2020/07/15/Linux-File/1.png" alt><br>第0位文件类型包括：<code>d</code>目录、<code>-</code>文件及其他（<code>l</code>、<code>b</code>、<code>c</code>），第1-3位表示该文件所有者的权限，第4-6位表示所有者同组用户的权限，第7-9位表示其他用户的权限。<code>r</code>读、<code>w</code>写、<code>x</code>执行（execute）表示拥有对应权限，<code>-</code>表示没有该权限。</p>
<h1 id="chown-R-属主名-文件或目录用于更改文件属主"><a href="#chown-R-属主名-文件或目录用于更改文件属主" class="headerlink" title="chown [-R] 属主名 文件或目录用于更改文件属主"></a><code>chown [-R] 属主名 文件或目录</code>用于更改文件属主</h1><p>加上参数<code>-R</code>表示递归更改文件属主。</p>
<h1 id="chgrp-R-属组名-文件或目录用于更改文件属组"><a href="#chgrp-R-属组名-文件或目录用于更改文件属组" class="headerlink" title="chgrp [-R] 属组名 文件或目录用于更改文件属组"></a><code>chgrp [-R] 属组名 文件或目录</code>用于更改文件属组</h1><p>加上参数<code>-R</code>表示递归更改文件属组。</p>
<h1 id="chmod-R-xyz-文件或目录用于更改文件的9个权限"><a href="#chmod-R-xyz-文件或目录用于更改文件的9个权限" class="headerlink" title="chmod [-R] xyz 文件或目录用于更改文件的9个权限"></a><code>chmod [-R] xyz 文件或目录</code>用于更改文件的9个权限</h1><p>加上参数<code>-R</code>表示递归更改文件权限。<br><code>xyz</code>表示要设置的权限数字，每个权限数字为3个权限分数之和，各权限分数对照如下：</p>
<ul>
<li><code>r</code>：4</li>
<li><code>w</code>：2</li>
<li><code>x</code>：1</li>
</ul>
<p>例如<code>7=rwx</code>，<code>0=---</code>，<code>drwxr-xr-x</code>的权限数字为<code>755</code>。</p>
<p>或者可以使用<code>chmod u=rwx,g=rx,o=r 文件名</code>的命令来设定文件的权限，具体如下表所示：</p>
<table>
<thead>
<tr>
<th>chmod</th>
<th>u</th>
<th>+（加入）</th>
<th>r</th>
<th>文件或目录</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>g</td>
<td>-（除去）</td>
<td>w</td>
<td></td>
</tr>
<tr>
<td></td>
<td>o</td>
<td>=（设定</td>
<td>x</td>
<td></td>
</tr>
<tr>
<td></td>
<td>a</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>Linux基础</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>操作系统知识——进程线程</title>
    <url>/2020/07/16/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B/</url>
    <content><![CDATA[<p>本文总结了进程与线程的基础知识。</p>
<a id="more"></a>

<h1 id="并发与并行"><a href="#并发与并行" class="headerlink" title="并发与并行"></a>并发与并行</h1><p>并发是指一段时间内在单核CPU上交替运行多个程序。</p>
<p>并行是指同一时刻在多核CPU上同时运行多个程序。</p>
<h1 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h1><p>进程是<strong>资源分配</strong>的基本单位。</p>
<p>进程间数据不共享，创建和销毁的开销较大。</p>
<h1 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h1><p>线程时<strong>独立调度</strong>的基本单位。</p>
<p>一个进程至少有一个线程，称为主线程，其他为子线程。</p>
<p>一个进程中可以有多个线程，它们共享进程资源。</p>
<h1 id="孤儿进程"><a href="#孤儿进程" class="headerlink" title="孤儿进程"></a>孤儿进程</h1><p>一个父进程退出，而它的子进程还在运行，这些子进程称为孤儿进程。孤儿进程将被init进程（进程号为1）所收养，并由init进程对它们完成状态收集工作。孤儿进程不会对系统造成伤害。</p>
<h1 id="僵尸进程"><a href="#僵尸进程" class="headerlink" title="僵尸进程"></a>僵尸进程</h1><p>如果子进程退出，而父进程没有调用wait()或waitpid()，那么子进程的进程描述符仍然存在系统中，这些子进程称为僵尸进程。系统能使用的进程号是有限的，若僵尸进程过多，可能导致系统无法产生新的进程。通过杀死父进程可以将僵尸进程变成孤儿进程，从而被init进程收养，只有init进程就会释放所有僵尸进程占有的资源，从而结束僵尸进程。</p>
<h1 id="进程状态的切换"><a href="#进程状态的切换" class="headerlink" title="进程状态的切换"></a>进程状态的切换</h1><p>进程有三个状态：</p>
<ul>
<li>就绪（ready）：等待被调度（等待CPU时间）</li>
<li>运行（running）</li>
<li>阻塞（waiting）：等待资源（除CPU时间外的资源）</li>
</ul>
<p>就绪和运行可以相互切换，其他都是单向切换。</p>
<h1 id="进程调度算法"><a href="#进程调度算法" class="headerlink" title="进程调度算法"></a>进程调度算法</h1><h2 id="批处理系统"><a href="#批处理系统" class="headerlink" title="批处理系统"></a>批处理系统</h2><ol>
<li><p><strong>先来先服务：</strong>非抢占式，有利于长作业，不利于短作业。</p>
</li>
<li><p><strong>短作业优先：</strong>非抢占式，长作业可能永远得不到调度。</p>
</li>
<li><p><strong>最短剩余时间优先：</strong>抢占式，按剩余运行时间的顺序进行调度。</p>
</li>
</ol>
<h2 id="交互式系统"><a href="#交互式系统" class="headerlink" title="交互式系统"></a>交互式系统</h2><ol>
<li><strong>时间片轮转：</strong>所有进程按先来先服务的原则排成队列，每次调度时把一个时间片分配给队首进程，队首进程时间片结束后被送到队尾。</li>
<li><strong>优先级调度：</strong>每个进程分配一个优先级，按优先级进行调度，为了避免低优先级进程永远得不到调度，可以随时间增加等待进程的优先级。</li>
<li><strong>多级反馈队列：</strong>设置多个时间片大小不同的队列，进程若在第一个队列结束时间片后未执行完毕，则被送到下一个队列末尾。</li>
</ol>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://cyc2018.github.io/CS-Notes/#/" target="_blank" rel="noopener">CS-Notes</a></p>
]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title>字节跳动安卓客户端面试题集</title>
    <url>/2020/07/17/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86/</url>
    <content><![CDATA[<p>本文搜集整理了网络上关于字节跳动客户端面试题。</p>
<a id="more"></a>

<ul>
<li><input disabled type="checkbox"> 线程同步的方式</li>
<li><input disabled type="checkbox"> 死锁产生的条件？如何避免和解决？</li>
<li><input disabled type="checkbox"> 为什么有了进程还要线程？</li>
<li><input disabled type="checkbox"> HTTP和HTTPS的区别？HTTPS连接建立的过程？如果服务端也需要客户端传证书，应该是连接建立的哪个阶段？HTTP用的是TCP还是UDP？</li>
<li><input disabled type="checkbox"> TCP和UDP的区别？应用场景？</li>
<li><input disabled type="checkbox"> TCP的拥塞控制</li>
<li><input disabled type="checkbox"> 分段和分页</li>
<li><input disabled type="checkbox"> 进程调度算法</li>
<li><input disabled type="checkbox"> 进程线程的区别与联系？进程之间的通信方式？线程之间通信方式？</li>
<li><input disabled type="checkbox"> 三次握手</li>
<li><input disabled type="checkbox"> SSL握手过程？第一次握手如何加密？第二次握手如何加密？对称加密？非对称加密？加密过程？</li>
<li><input disabled type="checkbox"> 信号量与互斥量的区别？</li>
<li><input disabled type="checkbox"> wait()和sleep()的区别？</li>
<li><input disabled type="checkbox"> 设计模式？</li>
<li><input disabled type="checkbox"> JAVA：LinkedList与ArrayList？HashMap扩容？ConcurrentHashMap？创建对象的几种方式？</li>
<li><input disabled type="checkbox"> JVM内存模型？</li>
<li><input disabled type="checkbox"> 活动的生命周期？活动的启动模式？Handler源码？</li>
<li><input disabled type="checkbox"> 在浏览器输入网址敲回车后经历了什么？a)三次握手；b)https的加密流程；c)对称加密与非对称加密原理（RSA、AES）</li>
<li><input disabled type="checkbox"> DNS原理</li>
</ul>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>字节跳动</tag>
        <tag>JAVA</tag>
        <tag>安卓客户端</tag>
        <tag>题集</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机网络——网络层</title>
    <url>/2020/07/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E5%B1%82/</url>
    <content><![CDATA[<p>本文整理计算机网络中运输层的主要知识点。</p>
<a id="more"></a>

<p><img src="/2020/07/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E5%B1%82/6.png" alt></p>
<p>网络层向上只提供尽最大努力交付的数据报服务，不提供服务质量的承诺。</p>
<p>路由器是网络层的中间设备。</p>
<h1 id="网际协议IP"><a href="#网际协议IP" class="headerlink" title="网际协议IP"></a>网际协议IP</h1><p><img src="/2020/07/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E5%B1%82/2.png" alt></p>
<h2 id="地址解析协议ARP"><a href="#地址解析协议ARP" class="headerlink" title="地址解析协议ARP"></a>地址解析协议ARP</h2><p>从网络层使用的IP地址解析出数据链路层使用的硬件地址。每个主机都有一个 ARP 高速缓存，里面有本局域网上的各主机和路由器的 IP 地址到 MAC 地址的映射表。</p>
<p>若主机 A 知道主机 B 的 IP 地址，但是 ARP 高速缓存中没有该 IP 地址到 MAC 地址的映射，此时主机 A 通过广播的方式发送 ARP 请求分组，主机 B 收到该请求后会发送 ARP 响应分组给主机 A 告知其 MAC 地址，随后主机 A 向其ARP高速缓存中写入主机 B 的 IP 地址到 MAC 地址的映射。</p>
<h2 id="网际控制报文协议ICMP"><a href="#网际控制报文协议ICMP" class="headerlink" title="网际控制报文协议ICMP"></a>网际控制报文协议ICMP</h2><p><img src="/2020/07/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E5%B1%82/8.png" alt></p>
<p>更有效地转发IP数据报和提高交付成功的机会。ICMP报文封装在IP数据报的数据部分。不是高层协议，而是IP层的协议。ICMP报文分为差错报告报文和询问报文。</p>
<p><img src="/2020/07/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E5%B1%82/9.png" alt></p>
<h3 id="PING-Packet-InterNet-Groper"><a href="#PING-Packet-InterNet-Groper" class="headerlink" title="PING(Packet InterNet Groper)"></a>PING(Packet InterNet Groper)</h3><p>PING是 ICMP 的一个重要应用，主要用来测试两台主机之间的连通性。</p>
<p>原理是向目的主机发送 ICMP回送请求报文，目的主机收到之后会发送回送回答报文。PING会根据时间和成功响应的次数估算出数据包往返时间以及丢包率。</p>
<h2 id="网际组管理协议IGMP"><a href="#网际组管理协议IGMP" class="headerlink" title="网际组管理协议IGMP"></a>网际组管理协议IGMP</h2><p>用于IP多播。</p>
<h1 id="IP编址方式"><a href="#IP编址方式" class="headerlink" title="IP编址方式"></a>IP编址方式</h1><h1 id="分类编址"><a href="#分类编址" class="headerlink" title="分类编址"></a>分类编址</h1><p><img src="/2020/07/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E5%B1%82/3.png" alt></p>
<p>将IP地址划分为若干个固定类，每一类地址由留个固定长度的字段组成：</p>
<p>$$ IP地址::={&lt;网络号&gt;,&lt;主机号&gt;} $$</p>
<ul>
<li>网络号：标志主机（或路由器）所连接到的网络。</li>
<li>主机号：标志主机（或路由器）。</li>
</ul>
<p>A、B、C类地址为单播地址（一对一通信），D类地址为多播地址（一对多通信），E类地址保留。</p>
<p><img src="/2020/07/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E5%B1%82/4.png" alt></p>
<h2 id="划分子网编址"><a href="#划分子网编址" class="headerlink" title="划分子网编址"></a>划分子网编址</h2><p>在主机号字段中拿一部分作为子网号，把两级 IP 地址划分为三级 IP 地址：<br>$$<br>IP地址::={&lt;网络号&gt;,&lt;子网号&gt;,&lt;主机号&gt;}<br>$$<br><img src="/2020/07/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E5%B1%82/5.png" alt></p>
<p>从IP数据报的首部无法看出源主机或目的主机是否进行了子网划分，因此需要用到子网掩码。B类IP地址的子网掩码是255.255.0.0，假定子网号占8位，则B类IP地址下的子网掩码为255.255.255.0。路由器通过将目的IP地址145.13.3.10和其相连的各网络子网掩码逐相“与”，得到要找的子网的网络地址145.13.3.0。</p>
<h2 id="无分类编址CIDR（构造超网）"><a href="#无分类编址CIDR（构造超网）" class="headerlink" title="无分类编址CIDR（构造超网）"></a>无分类编址CIDR（构造超网）</h2><p>使用网络前缀号和主机号来对 32位的IP 地址进行编码，网络前缀的长度可以根据需要变化。</p>
<p>$$IP地址::={&lt;网络前缀号&gt;,&lt;主机号&gt;}$$</p>
<p>CIDR记法在IP地址后面加上斜线，然后写上网络前缀所占的位数。网络前缀相同的连续IP地址组成一个CIDR地址块。CIDR的地址掩码可以继续称为子网掩码，子网掩码首1长度为网络前缀的长度。</p>
<p><img src="/2020/07/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E5%B1%82/7.png" alt></p>
<p>如图这种通过使用网络前缀来减少路由表项的方式称为路由聚合，也称为构造超网 。</p>
<p>在查找时可能会在路由表中得到不止一个网络前缀的匹配结果，应当采用最长前缀匹配来确定应该匹配那一个。</p>
<p>为了进行更有效的查找，通常把无分类编址的网络前缀路由表存放在二叉搜索树中。</p>
<h2 id="IP数据报的格式"><a href="#IP数据报的格式" class="headerlink" title="IP数据报的格式"></a>IP数据报的格式</h2><p><img src="/2020/07/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E5%B1%82/1.png" alt></p>
<p>20字节固定长度：</p>
<ul>
<li>版本：占4位，指IP协议的版本，通信双方版本必须一致。IPv4或IPv6。</li>
<li>首部长度：占4位，单位是4字节，表示首部最大长度为60字节，当首部长度不是4字节的整数倍时，必须利用填充字段加以填充。</li>
<li>区分服务：占1字节，用来获得更好的服务，一般不用。</li>
<li>总长度：占2字节，单位为字节，指首部和数据的总长度。</li>
<li>标识：占2字节，在数据报过长而发生分片时，相同数据报的不同分片具有相同标识符。</li>
<li>标志：占3位，第一位为1表示后面还有分片否则表示最后一个分片；第二位为1表示可以分片否则表示不能分片；第三位无意义。</li>
<li>片偏移：占13位，单位为8字节，表示分片在原数据报中的相对位置。每个分片的尺度一定是8字节的整数倍。</li>
<li>生存时间：占1字节，单位是跳数，数据报在网络中的寿命。</li>
<li>协议：占1字节，指出此数据报携带的数据是使用何种协议。</li>
<li>首部检验和：占2字节，检验数据报的首部，不包括数据部分。</li>
<li>原地址：占4字节。</li>
<li>目的地址：占4字节。</li>
</ul>
<h1 id="路由选择协议"><a href="#路由选择协议" class="headerlink" title="路由选择协议"></a>路由选择协议</h1><h2 id="内部网关协议RIP"><a href="#内部网关协议RIP" class="headerlink" title="内部网关协议RIP"></a>内部网关协议RIP</h2><p>路由信息协议RIP自洽系统内部进行路由选择，是一种分布式的基于距离向量的路由选择协议，距离是指跳数，直接相连的路由器跳数为 1。跳数最多为 15，超过 15 表示不可达。</p>
<p>RIP 按固定的时间间隔仅和相邻路由器交换自己的路由表，经过若干次交换之后，所有路由器最终会知道到达本自治系统中任何一个网络的最短距离和下一跳路由器地址。</p>
<p>RIP 协议实现简单，开销小。但是 RIP 能使用的最大距离为 15，限制了网络的规模。并且当网络出现故障时，要经过比较长的时间才能将此消息传送到所有路由器。</p>
<h2 id="内部网关协议OSPF"><a href="#内部网关协议OSPF" class="headerlink" title="内部网关协议OSPF"></a>内部网关协议OSPF</h2><p>开放最短路径优先协议OSPF在自洽系统内部进行路由选择，是公开发表的，使用了 Dijkstra 提出的最短路径算法 SPF。</p>
<p>OSPF 具有以下特点：</p>
<ul>
<li>向本自治系统中的所有路由器发送信息，这种方法是洪泛法。</li>
<li>发送的信息就是与相邻所有路由器的链路状态，链路状态包括与哪些路由器相连以及链路的度量。度量用费用、距离、时延、带宽等来表示。</li>
<li>只有当链路状态发生变化时，路由器才会发送信息。</li>
</ul>
<p>所有路由器都具有全网的拓扑结构图，并且是一致的。OSPF 的更新过程比RIP收敛得更快。</p>
<h2 id="外部网关协议BGP"><a href="#外部网关协议BGP" class="headerlink" title="外部网关协议BGP"></a>外部网关协议BGP</h2><p>边界网关协议BGP在自洽系统之间进行路由选择，只能力求寻找一条能够到达目的网络并且比较好的路径，而非寻找最佳路由。</p>
<p>每个自洽系统都必须选择一个路由器作为BGP发言人，通过在两个相邻BGP发言人之间建立TCP连接来交换路由信息。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>《计算机网络 第七版》谢希仁</p>
]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>网络层</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机网络——运输层</title>
    <url>/2020/07/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%BF%90%E8%BE%93%E5%B1%82/</url>
    <content><![CDATA[<p>本文整理计算机网络中运输层的主要知识点。</p>
<a id="more"></a>

<p><img src="/2020/07/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%BF%90%E8%BE%93%E5%B1%82/6.png" alt></p>
<p>TCP和UDP是TCP/IP运输层的两个主要协议。</p>
<p><img src="/2020/07/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%BF%90%E8%BE%93%E5%B1%82/7.png" alt></p>
<center><b>使用UDP和TCP协议的各种应用和应用层协议</b></center>

<h1 id="用户数据报协议UDP（User-Datagram-Protocol）"><a href="#用户数据报协议UDP（User-Datagram-Protocol）" class="headerlink" title="用户数据报协议UDP（User Datagram Protocol）"></a>用户数据报协议UDP（User Datagram Protocol）</h1><ol>
<li><p>传送数据前不需要建立连接。</p>
</li>
<li><p>尽最大努力交付。</p>
</li>
<li><p>面向报文，对应用程序交下来的报文，在添加UDP首部后就向下交付给IP层。</p>
</li>
<li><p>没有拥塞控制。</p>
</li>
<li><p>支持一对一、一对多、多对一、多对多通信。</p>
</li>
<li><p>8字节首部，开销小：源端口、目的端口、长度、检验和，每个字段都是两个字节。</p>
<p><img src="/2020/07/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%BF%90%E8%BE%93%E5%B1%82/1.png" alt></p>
</li>
</ol>
<h1 id="传输控制协议TCP（Transmission-Control-Protocol）"><a href="#传输控制协议TCP（Transmission-Control-Protocol）" class="headerlink" title="传输控制协议TCP（Transmission Control Protocol）"></a>传输控制协议TCP（Transmission Control Protocol）</h1><ol>
<li><p>传送数据前必须建立TCP连接。</p>
</li>
<li><p>每一条TCP连接只能是点对点（一对一）。</p>
</li>
<li><p>可靠交付。</p>
</li>
<li><p>全双工通信（双向同时通信）。</p>
</li>
<li><p>面向字节流。</p>
</li>
<li><p>最小20字节首部包括：</p>
<p><img src="/2020/07/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%BF%90%E8%BE%93%E5%B1%82/2.png" alt></p>
<ul>
<li>源端口和目标端口：各占2字节。</li>
<li>序号：占4字节，对字节流进行编号，如序号为301，表示第一个字节的编号为301，若携带数据长度为100字节，则下一个报文段的序号应该为401。4字节=32位，可对4GB数据进行编号。</li>
<li>确认号：占4字节，期望收到下一个报文段的序号。如服务器正确收到客户的一个序号为501的报文段，携带数据长度为200字节，因此服务器期望下一个报文段的序号为701，即发给客户的确认报文段中确认号为701。</li>
<li>数据偏移：占4位，指TCP首部的长度。单位是4字节，也就是说TCP首部最大长度为60字节。</li>
<li>保留：占6位，保留，目前置0。</li>
<li>紧急URG：占1位，为1时表示此报文段中有紧急数据，发送方应优先传送。</li>
<li>确认ACK：占1位，为1时确认号字段有效，TCP连接建立后所有报文段必须把ACK置1。</li>
<li>推送PSH：占1位，为1时接收方会尽快交付给接收进程。</li>
<li>复位RST：占1位，RST=1表示连接中存在严重差错，必须重新建立连接，或用于拒绝非法报文，或拒绝打开一个连接。</li>
<li>同步SYN：占1位，建立连接时用于同步序号。SYN=1，ACK=0表示连接请求报文段，若对方同意，则响应报文中SYN=1，ACK=1。</li>
<li>终止FIN：占1位，用于释放一个连接。FIN=1表示此报文段的发送方要求释放连接。</li>
<li>窗口：占2字节，告诉对方自己能接收的数据流字节长度。</li>
</ul>
</li>
</ol>
<h2 id="TCP的可靠传输"><a href="#TCP的可靠传输" class="headerlink" title="TCP的可靠传输"></a>TCP的可靠传输</h2><p>TCP使用超时重传来实现可靠传输，若在发送报文段后超时时间内没有收到确认，则重传这个报文段。</p>
<h2 id="TCP的滑动窗口协议"><a href="#TCP的滑动窗口协议" class="headerlink" title="TCP的滑动窗口协议"></a>TCP的滑动窗口协议</h2><p>发送方和接收方各有一个窗口用于缓存数据。发送缓存用于暂时存放准备发送的数据和已发送而尚未确认的数据。接收缓存用于暂时存放按序到达但尚未被接收进程读取的数据和未按序到达的数据。</p>
<h2 id="TCP的流量控制"><a href="#TCP的流量控制" class="headerlink" title="TCP的流量控制"></a>TCP的流量控制</h2><p>流量控制时为了控制发送方发送速率，保证接收方来得及接收。</p>
<p>接收方可以通过确认报文中的窗口字段来控制发送方的窗口大小，从而影响发送方的发送速率。</p>
<h2 id="TCP的拥塞控制方法"><a href="#TCP的拥塞控制方法" class="headerlink" title="TCP的拥塞控制方法"></a>TCP的拥塞控制方法</h2><p><img src="/2020/07/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%BF%90%E8%BE%93%E5%B1%82/3.png" alt></p>
<ol>
<li>慢开始：由小到大逐渐增大发送窗口（拥塞窗口），每收到一个确认，窗口长度（cwnd）×2，cwnd超过阈值ssthresh时，停止使用慢开始算法而改用拥塞避免算法（图中点①）。若出现超时，则设置ssthresh=cwnd/2，并重新执行慢开始算法（图中点②）。</li>
<li>拥塞避免：让发送窗口缓慢地增大，每收到一个确认，把cwnd加1。</li>
<li>快重传：接受方如果收到失序的报文段，要立即发出前一个报文段的重复确认，发送方只要一连收到3个重复确认，应立即执行快重传（图中点④），重新发送失序报文段。</li>
<li>快恢复：若发送方一连收到3个重复确认，知道只是丢失了个别字段而不是堵塞，于是不重新启动慢开始算法，而是执行快恢复算法，调整ssthresh=cwnd/2，同时设置cwnd=cwnd/2，并开始执行拥塞避免算法（图中点⑤）。</li>
</ol>
<h2 id="TCP的连接"><a href="#TCP的连接" class="headerlink" title="TCP的连接"></a>TCP的连接</h2><p>TCP连接的端点为套接字（socket）或插口：</p>
<p>$$ 套接字socket=（IP地址：端口号） $$</p>
<p>每一条TCP连接由两个套接字确定。发起连接的进程称为客户（client），被动等待连接的进程称为服务器（server）。</p>
<h3 id="TCP的连接建立（三次握手，三报文握手）"><a href="#TCP的连接建立（三次握手，三报文握手）" class="headerlink" title="TCP的连接建立（三次握手，三报文握手）"></a>TCP的连接建立（三次握手，三报文握手）</h3><p><img src="/2020/07/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%BF%90%E8%BE%93%E5%B1%82/4.png" alt></p>
<p>如图所示，TCP建立连接时需要在客户和服务器之间交换三个TCP报文段（客户请求，服务器确认，客户确认）。</p>
<ol>
<li>首先服务器端处于监听状态，等待客户的连接请求。</li>
<li>客户向服务器端发送连接请求报文。</li>
<li>服务器收到连接请求报文，如果同意连接，则向客户发送连接确认报文。</li>
<li>客户收到服务器的连接确认报文后，向服务器发出确认，且报文段可以携带数据。</li>
<li>服务器收到确认后，建立连接。</li>
</ol>
<p>第三次握手的原因：防止失效的连接请求到达服务器，让服务器错误打开连接。若客户的某个连接请求在网络中滞留时间过长，客户会重新请求连接，但滞留的连接请求还是会到达服务器，如果不进行第三次握手，那么服务器会打开两个连接。进行第三次握手使得最终连接权在客户。</p>
<h3 id="TCP的连接释放（四次挥手，四报文握手）"><a href="#TCP的连接释放（四次挥手，四报文握手）" class="headerlink" title="TCP的连接释放（四次挥手，四报文握手）"></a>TCP的连接释放（四次挥手，四报文握手）</h3><p><img src="/2020/07/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%BF%90%E8%BE%93%E5%B1%82/5.png" alt></p>
<p>TCP连接释放过程如图所示（客户释放，服务器确认，服务器释放，客户确认）。</p>
<ol>
<li>客户的进程向其TCP发出连接释放报文段，并停止发送数据，主动关闭TCP连接。</li>
<li>客户把连接释放报文段的FIN置1并发送给服务器，客户进入终止等待1状态，等待服务器的确认。</li>
<li>服务器收到连接释放报文段后发出确认，然后进入关闭等待状态。服务器通知进程，此时客户到服务器的连接释放了，但是服务器到客户的连接还未关闭。</li>
<li>客户收到服务器的确认后，进入终止等待2状态，继续等待服务器的连接释放报文。</li>
<li>若服务器没有要向客户端发送的数据，则向客户发出连接释放报文，必须使FIN=1，进入最后确认状态。</li>
<li>客户收到服务器的连接释放报文后，对此发出确认，进入时间等待状态，等待2MSL（最长报文寿命）后释放连接。</li>
<li>服务器收到确认报文后释放连接。</li>
</ol>
<p>客户等待2MSL的原因：</p>
<ol>
<li>为了保证客户的最后一个确认报文到达服务器，如果服务器没收到客户的确认报文，则会再次发送连接释放报文。</li>
<li>为了保证本次连接的所有报文已经从网络中消失。</li>
</ol>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>《计算机网络 第七版》谢希仁</p>
]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>运输层</tag>
      </tags>
  </entry>
  <entry>
    <title>模拟面试问答之经典网络模型四：YOLOv1</title>
    <url>/2020/07/17/yolov1/</url>
    <content><![CDATA[<p>本文总结了面试过程中可能问到的关于YOLOv1模型的一些问题。</p>
<p><a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">论文地址：You Only Look Once: Unified, Real-Time Object Detection</a></p>
<p><img src="/2020/07/17/yolov1/1.png" alt></p>
<center><b>图1 YOLOv1网络结构图</b></center>

<a id="more"></a>

<h1 id="简述一下YOLOv1测试时的检测流程？"><a href="#简述一下YOLOv1测试时的检测流程？" class="headerlink" title="简述一下YOLOv1测试时的检测流程？"></a>简述一下YOLOv1测试时的检测流程？</h1><ol>
<li>调整输入图像尺寸为448×448×3。</li>
<li>经过24层卷积输出7×7×1024特征图（64×下采样），再通过一个全连接层输出4096维特征，最后经过一个全连接层输出S×S×30的特征图。（Fast YOLO包括9个卷积）</li>
<li>通过NMS筛选最终结果，NMS的<code>score=某个类别的概率×置信度</code>。</li>
</ol>
<h1 id="YOLOv1的创新点有哪些？"><a href="#YOLOv1的创新点有哪些？" class="headerlink" title="YOLOv1的创新点有哪些？"></a>YOLOv1的创新点有哪些？</h1><ol>
<li>将检测问题转化为回归问题，同时输出预测框的坐标信息和类别概率。</li>
<li>没有提取候选区域的过程。</li>
</ol>
<h1 id="网络最后输出7×7×30特征图的含义？"><a href="#网络最后输出7×7×30特征图的含义？" class="headerlink" title="网络最后输出7×7×30特征图的含义？"></a>网络最后输出7×7×30特征图的含义？</h1><p>YOLOv1将图片划分为7×7个格子，每个格子负责预测中心点落在该格子区域的物体，每个格子预测两个矩形框。每个格子最后输出30个预测值，包括两个矩形框的位置信息<code>(x, y, w, h)</code>和置信度，以及20个类别的概率。</p>
<h1 id="YOLOv1是如何训练的？"><a href="#YOLOv1是如何训练的？" class="headerlink" title="YOLOv1是如何训练的？"></a>YOLOv1是如何训练的？</h1><ol>
<li><p>在ImageNet上对前20个卷积层进行预训练，具体地，训练时输入尺寸为224×224，网络结构为前20个卷积层+平均池化层+1000全连接层。</p>
</li>
<li><p>添加4个卷积层和两个全连接层，第一个全连接层后添加一个0.5的dropout层。输入尺寸为448×448，激活函数采用Leaky-ReLU（最后一层使用线性激活函数），优化方法采用0.9的Momentum，数据增强采用随机缩放，随机截取，随机调整曝光度和饱和度。</p>
</li>
<li><p>Loss函数由五个平方和误差（L2损失）组成：</p>
<p><img src="/2020/07/17/yolov1/2.png" alt></p>
<p>其中$\lambda_{coord}=5$，$\lambda_{noobj}=0.5$，由于含有物体的格子较少，因此加大含有物体的格子的损失贡献。</p>
<ul>
<li>第一行为与ground-truth的IoU较大的预测框的中心点坐标损失；</li>
<li>第二行为与ground-truth的IoU较大的预测框的宽高损失，取根号是为了消除大尺寸框与小尺寸框之间的差异；</li>
<li>第三行和第四行是两个预测框的置信度损失，若格子不含物体则置信度为0，否则为IoU；</li>
<li>第五行为格子的类别损失。</li>
</ul>
<p>对于有物体的格子，需要计算分类损失，两个框的置信度损失，IoU较大的框的位置损失。</p>
<p>对于没物体的格子，只需要计算两个框的置信度损失。</p>
</li>
</ol>
<h1 id="YOLOv1回归的坐标值是什么含义？"><a href="#YOLOv1回归的坐标值是什么含义？" class="headerlink" title="YOLOv1回归的坐标值是什么含义？"></a>YOLOv1回归的坐标值是什么含义？</h1><p>$(x,y)$是格子左上角坐标的偏移值（0-1），$(w,h)$预测的是预测框的宽高与原始图像宽高的比值（0-1）。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>YOLOv1具有运行速度快，背景误检率等优点，但由于其设置，每个格子最多只能检测一个物体，导致易漏检中心距离近的物体，召回率低，位置准确性较差。</p>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>经典网络</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>Python基础——Python 2.x与Python 3.x的主要区别</title>
    <url>/2020/07/17/Python%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%942-x%E4%B8%8E3-x%E7%9A%84%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p>本文总结了Python 2.x与Python 3.x的主要区别。</p>
<a id="more"></a>

<h1 id="print函数"><a href="#print函数" class="headerlink" title="print函数"></a>print函数</h1><p>print()函数取代print语句。</p>
<h1 id="Unicode"><a href="#Unicode" class="headerlink" title="Unicode"></a>Unicode</h1><p>Python 2.x默认使用ASCII编码，不能直接输出中文，变量命名只能用英文。</p>
<p>Python 3.x默认使用UTF-8编码，可以直接输出中文，可以使用中文变量名。</p>
<h1 id="除法运算"><a href="#除法运算" class="headerlink" title="除法运算/"></a>除法运算<code>/</code></h1><p>Python 2.x，整数相除结果为整数，浮点数相除为浮点数。</p>
<p>Python 3.x，结果始终是浮点数。</p>
<h1 id="打开文件"><a href="#打开文件" class="headerlink" title="打开文件"></a>打开文件</h1><p>Python 2.x可以使用file(…)或者open(…)。</p>
<p>Python 3.x只能使用open(…)。</p>
<h1 id="xrange函数"><a href="#xrange函数" class="headerlink" title="xrange函数"></a>xrange函数</h1><p>Python 3.x取消了xrange函数，使用range函数完全代替。</p>
<h1 id="八进制字面量表示"><a href="#八进制字面量表示" class="headerlink" title="八进制字面量表示"></a>八进制字面量表示</h1><p>Python 2.x可以用01000或0o1000表示八进制512。</p>
<p>Python 3.x只可以使用0o1000。</p>
<h1 id="不等运算符"><a href="#不等运算符" class="headerlink" title="不等运算符"></a>不等运算符</h1><p>Python 2.x可以用<code>!=</code>或<code>&lt;&gt;</code>。</p>
<p>Python 3.x只能使用<code>!=</code>。</p>
<h1 id="Python-3-x使用更加严格的缩进"><a href="#Python-3-x使用更加严格的缩进" class="headerlink" title="Python 3.x使用更加严格的缩进"></a>Python 3.x使用更加严格的缩进</h1><p>Python 2.x中允许tab和space共存。</p>
<p>Python 3.x只能单独使用tab或者space。</p>
<h1 id="去掉了repr表达式"><a href="#去掉了repr表达式" class="headerlink" title="去掉了repr表达式"></a>去掉了repr表达式</h1><p>Python 2.x中反引号``相当于repr函数的作用。</p>
<p>Python 3.x中去掉了``的写法，只允许使用repr函数。</p>
<h1 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h1><p>Python 3.x去掉了long类型，只有一种整形——int。新增了bytes类型。</p>
<h1 id="input和raw-input函数"><a href="#input和raw-input函数" class="headerlink" title="input和raw_input函数"></a>input和raw_input函数</h1><p>Python 2.x中raw_input会将所有输入数据当做字符串，返回值为字符串；而input输入时必须是一个合法的Python表达式。</p>
<p>Python 3.x只能使用input。</p>
<h1 id="map、filter和reduce"><a href="#map、filter和reduce" class="headerlink" title="map、filter和reduce"></a>map、filter和reduce</h1><p>Python 2.x中map，filter和reduce是内置函数，返回结果为列表。</p>
<p>Python 3.x中map和filter变成了类，返回结果为可迭代对象。reduce从全局挪到了functool模块中。</p>
<h1 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h1><p>捕获异常的语法由<code>except exc, var</code>改为<code>except exc as var</code>。</p>
<p>Python 2.x中所有类型的对象都可以被抛出；Python 3.x中只有继承自BaseException的对象才可以被抛出。</p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>模拟面试问答之经典网络模型五：YOLOv2</title>
    <url>/2020/07/18/yolov2/</url>
    <content><![CDATA[<p>本文总结了面试过程中可能问到的关于YOLOv2模型的一些问题。</p>
<p><a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">论文地址：YOLO9000: Better, Faster, Stronger</a><br><img src="/2020/07/18/yolov2/1.png" alt></p>
<center><b>图1 YOLOv2与YOLOv1技巧对比图</b></center>

<a id="more"></a>

<h1 id="YOLOv2采用了哪些技巧？"><a href="#YOLOv2采用了哪些技巧？" class="headerlink" title="YOLOv2采用了哪些技巧？"></a>YOLOv2采用了哪些技巧？</h1><ol>
<li>BN层代替Dropout层。</li>
<li>高分辨率的预训练：预训练时，先使用224×224的输入尺寸训练160个epochs，再使用448×448的输入尺寸训练10个epochs。</li>
<li>使用anchor boxes预测边界框：输入尺寸调整为416×416，输出特征图尺寸为13×13(32倍下采样），准确率下降但是召回率提高。</li>
<li>K-means聚类生成anchor box尺寸：聚类的距离函数为$d(box,centroid)=1-IoU(box,centroid)$，若采用欧氏距离会使得尺寸大的框误差也大。实验表明k=5时，在召回率和模型复杂的直接平衡性较好。</li>
<li>直接位置预测：预测相对锚框所属格子左上角坐标的偏移量$(b_x,b_y)$，以及与anchor box宽高的e的指数比例$(b_w,b_h)$，比RCNN系列采用的位置预测方法更易学习。</li>
<li>细粒度特征：类似ResNet，将最后一个26×26的特征图连接到最后一个13×13的特征图上。具体地，首先通过1×1卷积将26×26×512减低通道数为26×26×64，再拆分成4个13×13×64并拼接为13×13×256，然后与13×13×1024进行连接生成13×13×(256+1024)的特征图。</li>
<li>多尺度训练：每隔10个batch随机从{320,352,384,…,608}中选择一个尺度作为输入图像的尺寸。</li>
</ol>
<h1 id="YOLOv2的网络结构有哪些改进？"><a href="#YOLOv2的网络结构有哪些改进？" class="headerlink" title="YOLOv2的网络结构有哪些改进？"></a>YOLOv2的网络结构有哪些改进？</h1><p><img src="/2020/07/18/yolov2/2.png" alt></p>
<center><b>图2 Darknet19网络结构图</b></center>

<ol>
<li><p>预训练时，如图2所示，Darknet 19包含19个卷积层（YOLOv1网络包含24个卷积层和2个全连接层）。</p>
<p><img src="/2020/07/18/yolov2/3.png" alt></p>
<center><b>图2 YOLOv2网络结构图</b></center>
</li>
<li><p>用于检测时，如图3所示，首先添加三个3×3的卷积，然后输出特征图与细粒度特征中提到的跳跃连接进行拼接，再通过一个3×3卷积调整通道数，最后通过一个1×1卷积输出13×13×125的检测结果，对于13×13个格子，每个格子输出5×25维向量，其中5表示每个格子有五个anchor boxes，25包括20个类别概率，4个位置预测和1个置信度。</p>
</li>
<li><p>预训练时，先使用224×224的输入尺寸训练160个epochs，再使用448×448的输入尺寸训练10个epochs。</p>
</li>
</ol>
<h1 id="YOLOv2的样本是如何设置的？"><a href="#YOLOv2的样本是如何设置的？" class="headerlink" title="YOLOv2的样本是如何设置的？"></a>YOLOv2的样本是如何设置的？</h1><p>类似与YOLOv1，对于一个ground-truth，包含其中心点的格子的5个anchor boxes负责预测该物体框，只有与ground-truth的IoU最大的anchor box会用于计算loss。YOLOv2不再对宽高取根号。与ground-truth的IoU最大或大于0.6的anchor box为正样本。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://blog.csdn.net/zijin0802034/article/details/77097894" target="_blank" rel="noopener">https://blog.csdn.net/zijin0802034/article/details/77097894</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/35325884" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35325884</a></p>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>经典网络</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>模拟面试问答之经典网络模型六：YOLOv3</title>
    <url>/2020/07/18/yolov3/</url>
    <content><![CDATA[<p>本文总结了面试过程中可能问到的关于YOLOv3模型的一些问题。</p>
<p><a href="https://arxiv.org/abs/1804.02767" target="_blank" rel="noopener">论文地址：YOLOv3: An Incremental Improvement</a><br><img src="/2020/07/18/yolov3/1.png" alt></p>
<center><b>图1 Darknet-53网络结构图</b></center>

<a id="more"></a>

<h1 id="YOLOv3在有哪些主要改进？"><a href="#YOLOv3在有哪些主要改进？" class="headerlink" title="YOLOv3在有哪些主要改进？"></a>YOLOv3在有哪些主要改进？</h1><ol>
<li><p>Darknet-53代替Darknet-19，使用了残差模块，步长为2的卷积层代替池化层进行下采样。</p>
<p><img src="/2020/07/18/yolov3/3.jpg" alt></p>
<center><b>图2 YOLOv3网络结构图</b></center>
</li>
<li><p>采用了多尺度结构，在获得32倍下采样特征图后，又开始对特征图进行上采样，并与前面的16倍下采样特征图连接，然后继续对特征图进行上采样，并与前面的8倍下采样特征图连接。分别在三个尺度（8,16,32）的特征图上进行检测，输出分别为13×13×3×(4+1+80)，26×26×3×(4+1+80)，52×52×3×(4+1+80)。</p>
</li>
<li><p>使用独立的logistic分类器（sigmoid）代替softmax分类器进行分类。</p>
</li>
</ol>
<h1 id="YOLOv3在anchor-boxes方面有什么改进？"><a href="#YOLOv3在anchor-boxes方面有什么改进？" class="headerlink" title="YOLOv3在anchor boxes方面有什么改进？"></a>YOLOv3在anchor boxes方面有什么改进？</h1><p>由于YOLOv3采用了多尺度预测，anchor boxes的设计也做了相应改进。同样是使用k-means进行聚类，YOLOv3聚类了3个尺度，每个尺度聚类了3种尺寸的anchor boxes。</p>
<h1 id="YOLOv3的损失函数有哪些改进？"><a href="#YOLOv3的损失函数有哪些改进？" class="headerlink" title="YOLOv3的损失函数有哪些改进？"></a>YOLOv3的损失函数有哪些改进？</h1><p>置信度和类别用交叉熵损失替换了平方和损失。为了提高小物体的损失贡献，位置回归损失会乘以系数(2-w×h)。</p>
<p><img src="/2020/07/18/yolov3/2.png" alt></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://blog.csdn.net/leonardohaig/article/details/90346325" target="_blank" rel="noopener">https://blog.csdn.net/leonardohaig/article/details/90346325</a></p>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>经典网络</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>模拟面试问答之经典网络模型七：SSD</title>
    <url>/2020/07/19/SSD/</url>
    <content><![CDATA[<p>本文总结了面试过程中可能问到的关于SSD模型的一些问题。</p>
<p><a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">论文地址：Single Shot MultiBox Detector</a></p>
<p><img src="/2020/07/19/SSD/1.png" alt></p>
<center><b>图1 SSD与YOLOv1网络结构对比图</b></center>

<a id="more"></a>

<h1 id="SSD有哪些主要特点？"><a href="#SSD有哪些主要特点？" class="headerlink" title="SSD有哪些主要特点？"></a>SSD有哪些主要特点？</h1><ol>
<li><p>类似YOLO，将检测转化为回归，一次性完成定位与回归。</p>
</li>
<li><p>类似Faster RCNN，使用了先验框。</p>
<p><img src="/2020/07/19/SSD/2.png" alt></p>
<center><b>图2 SSD详细网络结构图</b></center>
</li>
<li><p>利用了多尺度特征进行检测（300–&gt;38,19,10,5,3,1）。</p>
</li>
</ol>
<h1 id="SSD网络的输出是什么含义？"><a href="#SSD网络的输出是什么含义？" class="headerlink" title="SSD网络的输出是什么含义？"></a>SSD网络的输出是什么含义？</h1><p>SSD包括6个输出，分别是38×38×4×(4+21)，19×19×4×(4+21)，10×10×6×(4+21)，5×5×6×(4+21)，3×3×6×(4+21)，1×1×4×(4+21)，其中(4,4,6,6,6,4)表示该尺度下，每个格子预测的先验框数量，4表示预测的位置信息，21表示20个类别+背景的置信度。</p>
<h1 id="SSD的先验框是如何设置的？"><a href="#SSD的先验框是如何设置的？" class="headerlink" title="SSD的先验框是如何设置的？"></a>SSD的先验框是如何设置的？</h1><ol>
<li>以特征图每个格子的中心坐标为中心，生成一系列的同心先验框。</li>
<li>设置了m=6个尺度的先验框，最底层特征图上的尺度为$S_{min}=0.2$，最高层的为$S_{max}=0.9$，其余层在0.2到0.9之间均等取值。$S_K$表示先验框与输入图像的比例。</li>
<li>每个尺度设置了4或6种（4,6,6,6,4,4）不同的长宽比$a_r\in{1,2,3,1/2,1/3}$，可以计算第k层的先验框的宽为$w_k^a=s_k\sqrt{a_r}$，高为$h_k^a=s_k/\sqrt{a_r}$，此外还有一个边长为$\sqrt{S_kS_{k+1}}$的正方形框，因此每个格子产生6种框。</li>
</ol>
<h1 id="SSD中正负样本如何设定的？"><a href="#SSD中正负样本如何设定的？" class="headerlink" title="SSD中正负样本如何设定的？"></a>SSD中正负样本如何设定的？</h1><ol>
<li>对于每个ground-truth，找到与其IoU最大的先验框设为正样本，这样可以保证每个ground-truth都有一个先验框相匹配。</li>
<li>对于剩余未匹配的先验框，若与某个ground-truth的IoU大于阈值（0.5），则先验框为正样本，其余为负样本。这意味着某个ground-truth可能与多个先验框相匹配（YOLO中，一个ground-truth只有一个对应的先验框）。</li>
<li>训练时，将负样本按背景置信度（预测背景的置信度越小，误差越大）进行排序，选取最低的k个，保证正负样本比例为1:3。</li>
</ol>
<h1 id="SSD的是如何训练的？"><a href="#SSD的是如何训练的？" class="headerlink" title="SSD的是如何训练的？"></a>SSD的是如何训练的？</h1><ol>
<li><p>在ILSVRC上预训练VGG16。</p>
</li>
<li><p>类似于DeepLabv1，将VGG16的前两个全连接层替换为卷积层，并加入了空洞卷积。</p>
</li>
<li><p>采用0.9的Momentum。</p>
</li>
<li><p>采用水平翻转，随机裁剪，颜色扭曲，随机采取区域等数据增强方法。</p>
</li>
<li><p>损失函数由位置误差和分类置信度误差组成：</p>
<p><img src="/2020/07/19/SSD/3.png" alt></p>
<p>位置误差采用Smooth L1损失，预测的$(\Delta x, \Delta y,S_w,S_h)$与RCNN系列中的定义一致。分类置信度误差采用softmax损失（交叉熵损失）。</p>
</li>
</ol>
<h1 id="测试时是SSD如何进行预测的？"><a href="#测试时是SSD如何进行预测的？" class="headerlink" title="测试时是SSD如何进行预测的？"></a>测试时是SSD如何进行预测的？</h1><ol>
<li>对于每个预测框，首先根据类别置信度确定其类别，并过滤掉置信度小于阈值或属于背景的预测框。</li>
<li>按置信度进行排序，保留top-k个预测框。</li>
<li>根据预测框的位置信息对预测框进行修正。</li>
<li>使用NMS过滤掉重叠较大的预测框。</li>
</ol>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://blog.csdn.net/thisiszdy/article/details/89576389" target="_blank" rel="noopener">https://blog.csdn.net/thisiszdy/article/details/89576389</a></p>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>经典网络</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机网络——应用层</title>
    <url>/2020/07/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E5%BA%94%E7%94%A8%E5%B1%82/</url>
    <content><![CDATA[<p>本文整理计算机网络中应用层的主要知识点。</p>
<a id="more"></a>

<p><img src="/2020/07/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E5%BA%94%E7%94%A8%E5%B1%82/6.png" alt></p>
<h1 id="DNS——域名系统"><a href="#DNS——域名系统" class="headerlink" title="DNS——域名系统"></a>DNS——域名系统</h1><p>主机不仅有IP地址，还有便于用户记忆的主机名字。DNS能够将互联网上的主机名字转换为IP地址。</p>
<ul>
<li>域名具有层次结构：</li>
</ul>
<p><img src="/2020/07/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E5%BA%94%E7%94%A8%E5%B1%82/2.png" alt></p>
<ul>
<li>级别最低的域名写在最左边，级别最高的域名写在最右边：</li>
</ul>
<p><img src="/2020/07/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E5%BA%94%E7%94%A8%E5%B1%82/1.png" alt></p>
<ul>
<li><p>域名到IP的解析过程：</p>
<ol>
<li><p>应用进程调用解析程序，并成为DNS的一个客户，把待解析的域名放在DNS请求报文中，以UDP或TCP（一般用UDP）用户数据报方式发给本地域名服务器。</p>
</li>
<li><p>若本地域名服务器能解析该域名，把对应的IP地址放在回答报文中返回。</p>
</li>
<li><p>若本地域名服务器不能回答该请求，则此域名服务器就暂时成为DNS中的另一个客户，并向其他域名服务器发出查询请求：</p>
<ul>
<li>迭代查询（常用）：其他域名服务器收到域名服务器的查询请求报文时，返回IP或告诉本地域名服务器下一步应该向哪一个域名服务器查询。</li>
<li>递归查询：其他域名服务器收到域名服务器的查询请求报文时，返回IP或成为DNS客户继续向其他域名服务器查询。</li>
</ul>
<p>其他域名服务器采用哪种查询方式取决于最初的查询报文的设置。</p>
</li>
</ol>
</li>
<li><p>域名服务器和主机中广泛地使用了高速缓存来存放最近查询的域名以及从何处获得域名映射信息的记录。</p>
</li>
</ul>
<h1 id="FTP——文件传送协议"><a href="#FTP——文件传送协议" class="headerlink" title="FTP——文件传送协议"></a>FTP——文件传送协议</h1><ul>
<li>FTP使用客户服务器方式。FTP服务器进程包括两大部分：<ul>
<li>一个进程：负责接受新的请求。</li>
<li>若干个从属进程：负责处理单个请求。</li>
</ul>
</li>
<li>FTP使用TCP进行连接，需要两个TCP连接（控制连接和文件传送连接）来传送一个文件。</li>
<li>主进程工作步骤如下：<ol>
<li>打开端口熟知端口21，等待客户发出连接请求。</li>
<li>启动从属进程处理客户的请求。<ul>
<li>控制进程：接收到客户通过“控制连接”发送的文件传送请求后，利用自己的熟知端口20与客户提供的端口号建立“数据连接”，并打开“数据传送进程”。</li>
<li>数据传送进程：用于传送文件数据，传送完毕后关闭。</li>
</ul>
</li>
<li>回到等待状态。</li>
</ol>
</li>
<li>简单文件传送协议TPTP使用UDP连接。</li>
</ul>
<h1 id="TELNET——远程终端协议"><a href="#TELNET——远程终端协议" class="headerlink" title="TELNET——远程终端协议"></a>TELNET——远程终端协议</h1><p>TELNET通过TCP连接登录，能将客户的击键传送到服务器，同时也能将服务器的输出返回到客户屏幕，可以适应许多计算机和操作系统的差异。和FTP情况类似，服务器中的主进程等待新的请求，并产生从属进程来处理每一个连接。</p>
<h1 id="电子邮件协议"><a href="#电子邮件协议" class="headerlink" title="电子邮件协议"></a>电子邮件协议</h1><ul>
<li><p>一个电子邮件系统由三部分组成：</p>
<ul>
<li>用户代理：用户与电子邮件系统的接口，即客户端软件。</li>
<li>邮件服务器：发送和接收邮件等。</li>
<li>邮件协议：发送协议SMTP，读取协议POP3或IMAP，都是使用TCP连接。</li>
</ul>
</li>
<li><p>电子邮件发送和接收步骤：</p>
<p><img src="/2020/07/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E5%BA%94%E7%94%A8%E5%B1%82/3.png" alt></p>
<ol>
<li><p>用户代理用SMTP协议将邮件发给邮件服务器。</p>
</li>
<li><p>邮件服务器将邮件临时存放在邮件缓存队列中，等待发送到邮件服务器。</p>
</li>
<li><p>发送方邮件服务器的SMTP客户与接收方邮件服务器的SMTP服务器建立TCP连接，然后把邮件缓存队里中的邮件依次发出去。</p>
</li>
<li><p>接收方的SMTP服务器把邮件放入收件人的用户邮箱。</p>
</li>
<li><p>收件人使用POP3（或IMAP）协议读取邮件。</p>
</li>
</ol>
</li>
</ul>
<h2 id="SMTP——简单邮件传送协议"><a href="#SMTP——简单邮件传送协议" class="headerlink" title="SMTP——简单邮件传送协议"></a>SMTP——简单邮件传送协议</h2><p>SMTP 只能发送 ASCII 码，而互联网邮件扩充 MIME 可以发送二进制文件。MIME 并没有改动或者取代 SMTP，而是增加邮件主体的结构，定义了非 ASCII 码的编码规则。</p>
<h2 id="POP3——邮局协议"><a href="#POP3——邮局协议" class="headerlink" title="POP3——邮局协议"></a>POP3——邮局协议</h2><p>POP3 的特点是只要用户从POP3服务器上读取了邮件，POP3就把该邮件删除。功能扩充的 POP3 可以设置读取后的存放时间。</p>
<h2 id="IMAP——网际报文存取协议"><a href="#IMAP——网际报文存取协议" class="headerlink" title="IMAP——网际报文存取协议"></a>IMAP——网际报文存取协议</h2><p>IMAP 协议中客户端和服务器上的邮件保持同步，如果不手动删除邮件，那么服务器上的邮件也不会被删除。IMAP 这种做法可以让用户随时随地去访问服务器上的邮件。</p>
<h1 id="DHCP——动态主机配置协议"><a href="#DHCP——动态主机配置协议" class="headerlink" title="DHCP——动态主机配置协议"></a>DHCP——动态主机配置协议</h1><p>DHCP提供了一种即插即用连网的机制，自动配置IP地址、子网掩码和网关IP地址。</p>
<p>工作过程：</p>
<p><img src="/2020/07/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E5%BA%94%E7%94%A8%E5%B1%82/4.png" alt></p>
<ol>
<li>需要IP地址的主机以广播方式发送发现报文，该报文目的地址为255.255.255.255:67，源地址为0.0.0.0:68，被放入UDP中。</li>
<li>DHCP中继代理（通常是一台路由器）收到发现报文后，以单播方式向DHCP服务器转发此报文。</li>
<li>收到DHCP服务器回答的提供报文后，DHCP中继代理再把此提供报文发回给主机。</li>
</ol>
<h1 id="HTTP——超文本传送协议"><a href="#HTTP——超文本传送协议" class="headerlink" title="HTTP——超文本传送协议"></a>HTTP——超文本传送协议</h1><p>HTTP协议是客户和服务器之间进行请求和应答的格式和规范。</p>
<p>HTTP使用了TCP作为运输层协议，但是HTTP本身是<strong>无连接的</strong>，也就是说通信双方在交换HTTP报文之前不需要先建立HTTP连接。</p>
<p>HTTP协议时<strong>无状态的</strong>，也就是说同一个客户第二次访问同一个服务器的页面时，服务器的响应与第一次访问相同。</p>
<p>在浏览器地址栏键入URL，按下回车之后会经历以下流程：</p>
<ol>
<li>浏览器向本地DNS服务器请求解析该URL中的域名对应的IP地址。</li>
<li>根据该IP地址和默认端口80，与服务器建立TCP连接，HTTP请求报文会被放入TCP连接三次握手的第三个报文数据中发给服务器。</li>
<li>服务器对浏览器请求做出响应，并把所请求的文档作为响应报文返回给浏览器。</li>
<li>HTTP/1.0立即释放TCP连接，HTTP/1.1在一段时间内保持该连接。</li>
<li>浏览器显示文档内容。</li>
</ol>
<h2 id="HTTP报文结构"><a href="#HTTP报文结构" class="headerlink" title="HTTP报文结构"></a>HTTP报文结构</h2><p><img src="/2020/07/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E5%BA%94%E7%94%A8%E5%B1%82/5.png" alt></p>
<p>HTTP有两类报文：</p>
<ul>
<li><p>请求报文：从客户向服务器发送请求报文。</p>
<ul>
<li>请求报文常用的方法：</li>
</ul>
<p><img src="/2020/07/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E5%BA%94%E7%94%A8%E5%B1%82/7.png" alt></p>
</li>
<li><p>响应报文：从服务器到客户的回答。</p>
<ul>
<li>响应报文状态码包括：<ul>
<li>1xx表示通知信息，如请求收到了或正在进行处理。</li>
<li>2xx表示成功，如接受或者知道了。常用<code>HTTP/1.1 202 Accepted {接受}</code>。</li>
<li>3xx表示重定向，如要完成请求还必须采取进一步行动。常用<code>HTTP//1.1 301 Moved Permanently {永久性地转移了}</code>。</li>
<li>4xx表示客户的差错，如请求中有错误的语法或不能完成。常用<code>HTTP/1.1 400 Bad Request {错误的请求}</code>，<code>HTTP/1.1 404 Not Found {找不到}</code> 。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>《计算机网络 第七版》谢希仁</p>
]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>应用层</tag>
      </tags>
  </entry>
</search>
